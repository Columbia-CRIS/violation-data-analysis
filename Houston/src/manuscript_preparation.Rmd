---
title: "Manuscript Preparation"
author: "Albert"
date: "4/13/2018"
output: 
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE)
options(scipen = 1, digits = 2)
require(dplyr)
require(ggplot2)
require(RcppRoll)
#require(markovchain) # FIX: markovchain seems not compatible with R3.5.0
require(reshape)
require(survival)
require(caret)
require(e1071)
require(randomForest)
require(kernlab)
require(SuperLearner)
require(ranger)

## Abhishek Macbook
# knitr::opts_knit$set(root.dir = "~/Documents/Projects/Github/CRIS/ViolationData/violation-data-analysis/") # !! CHANGE PATH TO MATCH YOUR OWN
# source("~/Documents/Projects/Github/CRIS/ViolationData/violation-data-analysis/Houston/src/prepare_violation.R")
# source("~/Documents/Projects/Github/CRIS/ViolationData/violation-data-analysis/Houston/src/roll_over.R")

##Abhishek Desktop
# knitr::opts_knit$set(root.dir = "D:/Github/CRIS/violation-data-analysis/") # !! CHANGE PATH TO MATCH YOUR OWN
# source("D:/Github/CRIS/violation-data-analysis/Houston/src/prepare_violation.R")
# source("D:/Github/CRIS/violation-data-analysis/Houston/src/roll_over.R")

##Albert Macbook
knitr::opts_knit$set(root.dir = "~/documents/projects/violation-data-analysis") # !! CHANGE PATH TO MATCH YOUR OWN

```

## Data Preparation

### Load data

Loading raw data.    

```{r load data, include=FALSE}
source("./Houston/src/prepare_violation.R")
source("./Houston/src/roll_over.R")
if(!exists("Accidents") | 
   !exists("AssessedViolations") | 
   !exists("Mines")) {
  load("./Houston/data/Accidents.RData")
  load("./Houston/data/AssessedViolations.RData")
  load("./Houston/data/Mines.RData")
  print("raw RData loaded")
}
```

Let's examine the loaded raw datasets.

```{r data preview, include=FALSE}
# Attributes in Accidents Table
head(Accidents)

# Attributes in Violations Table
head(AssessedViolations)

# Attributes in Mines Table
head(Mines)
```

The `result.RData` from San Antonio folder is the same as the `Consolidated.RData` in Houston folder.
     
```{r load result, include=FALSE}
if(!exists("complete.active.quarters")){load("./San Antonio/output/result.RData")}
print("load previous consolidated data")

colnames(complete.active.quarters)
```

### Data Visualization

If we define severe as death plus permenant disability, then severe composition is shown as following.    

```{r severe1}
# options(digits = 1)
if(!exists("complete.active.quarters")){load("./San Antonio/output/result.RData")}
severe.summary <- complete.active.quarters %>%
  filter(active) %>%
  mutate(severe = ifelse(
    num.death + num.dis > 0, TRUE, FALSE
  )) %>%
  group_by(severe) %>%
  summarise(n = n()) %>%
  mutate(perc = n / sum(n) * 100)
print(severe.summary)
```

If we redefine severe as death or permenant diability greater than 0, or more than 300 days lost or days retrict, then the severe composition is following.
```{r severe2}
# options(digits = 1)
if(!exists("complete.active.quarters")){load("./San Antonio/output/result.RData")}
severe.summary <- complete.active.quarters %>%
  filter(active) %>%
  mutate(severe = ifelse(
    (num.death + num.dis > 0) | (num.days.lost + num.days.restrict > 300), TRUE, FALSE
  )) %>%
  group_by(severe) %>%
  summarise(n = n()) %>%
  mutate(perc = n / sum(n) * 100)
print(severe.summary)
```
   
## Model Development

### Linear Regression
    
Read these two links first: [How to interpret regression p-values](http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients) and
[How to interpret the output of the summary() method for an lm object](http://stats.stackexchange.com/questions/59250/how-to-interpret-the-output-of-the-summary-method-for-an-lm-object-in-r)

We first examine the linearity between "number of days lost" and "last three years' number of violations". 

```{r check linearity, include=FALSE}
ggplot(data=complete.active.quarters, mapping = aes(x=last.three.years.viol,y=num.days.lost))+geom_point()
```
It turns out that these variables do not show linear correlation, which violates the linear regression assumption.

Linear regression on num of days lost or restricted:

```{r, include=FALSE}
if(!exists("complete_active_quarters")){load("./Houston/output/Consolidated.RData")}
colnames(complete_active_quarters)

num.days.lm <- lm(0*num.death+1*num.days.lost+1*num.days.restrict~last.quarter.lost+last.year.lost+last.three.years.lost
           +last.quarter.restrict+last.year.restrict+last.three.years.restrict
           +last.quarter.viol+last.year.viol+last.three.years.viol
           +last.quarter.death+last.year.death+last.three.years.death, 
           data=complete.active.quarters %>% filter(active))
summary(num.days.lm)
```
Linear Regression on num of deaths:

```{r echo=FALSE}
summary(lm(num.death~last.three.years.lost+last.quarter.viol+last.year.viol, data=complete.active.quarters %>% filter(active)))
```

### Logistic Regression

```{r logistic reg, include=FALSE}
# label data ----
labeled.data.death <- complete.active.quarters %>% filter(active) %>% mutate(
  severe = ifelse(
    num.death + num.dis > 0, TRUE, FALSE
  )
)
labeled.data.lost <- complete.active.quarters %>% filter(active) %>% mutate(
  severe = ifelse(
    num.death + num.dis > 0 | num.days.lost > 300, TRUE, FALSE
  )
)

# logistic regression without fixed effects ----
# regular logistic regression model
glm.model <- glm(severe ~ 
                   last.quarter.lost+last.year.lost+last.three.years.lost
                 +last.quarter.restrict+last.year.restrict+last.three.years.restrict
                 +last.quarter.viol+last.year.viol+last.three.years.viol
                 +last.quarter.death+last.year.death+last.three.years.death,
                 data = labeled.data.death, family = binomial(link='logit'))
summary(glm.model)

glm.prediction <- predict(object = glm.model, type = "response")
glm.result <- labeled.data.death %>% select(mine_id, mine.name, year, quarter, num.death, num.dis, severe)
glm.result$prob <- glm.prediction
glm.result <- glm.result %>% mutate(
  pred = ifelse(
    prob > 0.5, TRUE, FALSE
  )
)
head(glm.result %>% filter(year >= 2005) %>% arrange(desc(num.death), mine.name), 10)
head(glm.result %>% filter(year >= 2005) %>% arrange(desc(prob), mine.name), 10)

glm.performance <- confusionMatrix(data = factor(glm.result$pred), reference = factor(glm.result$severe), positive = "TRUE")
print(glm.performance)	
```

### Conditional Logistic Regression

```{r fixed effect, include=FALSE}
# fixed effects on training and testing data ----
# train and test
set.seed(1)
data.len <- nrow(labeled.data.death)
train.indices <- sample(seq_len(nrow(labeled.data.death)), size = floor(data.len / 2))

labeled.data.death.train <- labeled.data.death[train.indices, ]
labeled.data.death.result <- labeled.data.death[-train.indices, ]
labeled.data.death.result <- labeled.data.death.result %>% filter(is.element(mine_id, unique(labeled.data.death.train$mine_id)))

fe.div.model <- clogit(severe ~ last.quarter.lost+last.year.lost+last.three.years.lost
                   +last.quarter.restrict+last.year.restrict+last.three.years.restrict
                   +last.quarter.viol+last.year.viol+last.three.years.viol
                   +last.quarter.death+last.year.death+last.three.years.death
                   + strata(mine_id), data = labeled.data.death.train)
summary(fe.div.model)

fe.div.prediction <- predict(object = fe.div.model, newdata = labeled.data.death.result,type = "lp") %>% sigmoid()
fe.div.result <- labeled.data.death.result %>% select(mine_id, mine.name, year, quarter, num.death, num.dis, severe)
fe.div.result$prob <- fe.div.prediction
fe.div.result <- fe.div.result %>% mutate(
  pred = ifelse(
    prob > 0.5, TRUE, FALSE
  )
)
head(fe.div.result %>% filter(year >= 2005) %>% arrange(desc(num.death), mine.name), 10)
head(fe.div.result %>% filter(year >= 2005) %>% arrange(desc(prob), mine.name), 10)

# confusion matrix and other performance metrics
#fe.div.performance <- confusionMatrix(data = fe.div.result$pred, reference = fe.div.result$severe, positive = "TRUE")
fe.div.performance <- table(fe.div.result$pred, fe.div.result$severe)
print(fe.div.performance)

# fixed effects on training and testing data (using labeled.data.lost) ----
# fe.div + num.days.lost: train and test
labeled.data.lost.train <- labeled.data.lost[train.indices, ]
labeled.data.lost.result <- labeled.data.lost[-train.indices, ]
labeled.data.lost.result <- labeled.data.lost.result %>% filter(is.element(mine_id, unique(labeled.data.lost.train$mine_id)))

fe.div2.model <- clogit(severe ~ last.quarter.lost+last.year.lost+last.three.years.lost
                       +last.quarter.restrict+last.year.restrict+last.three.years.restrict
                       +last.quarter.viol+last.year.viol+last.three.years.viol
                       +last.quarter.death+last.year.death+last.three.years.death
                       + strata(mine_id), data = labeled.data.lost.train)
summary(fe.div2.model)

fe.div2.prediction <- predict(object = fe.div2.model, newdata = labeled.data.lost.result,type = "lp") %>% sigmoid()
fe.div2.result <- labeled.data.lost.result %>% select(mine_id, mine.name, year, quarter, num.death, num.dis, severe)
fe.div2.result$prob <- fe.div2.prediction
fe.div2.result <- fe.div2.result %>% mutate(
  pred = ifelse(
    prob > 0.5, TRUE, FALSE
  )
)
# head(fe.div2.result %>% filter(year >= 2005) %>% arrange(desc(num.death), mine.name), 10)
# head(fe.div2.result %>% filter(year >= 2005) %>% arrange(desc(prob), mine.name), 10)

# confusion matrix and other performance metrics
fe.div2.performance <- confusionMatrix(data = factor(fe.div2.result$pred), reference = factor(fe.div2.result$severe), positive = "TRUE")
print(fe.div2.performance$table)
print(c(fe.div2.performance$overall["Accuracy"], fe.div2.performance$byClass[c("Sensitivity", "Specificity", "Precision", "F1")]))   
```

### Clustering

#### K-means

First, we cluster the data using K-means.    

```{r clustering, include=FALSE}
# clustering ----
temp <- complete.active.quarters %>% filter(active==TRUE)
set.seed(20)
clustering <- kmeans(temp[, 6:ncol(temp)], 3, nstart=20)
freq_table <- table(clustering$cluster)

#save(clustering, file="kmeans_clustering.RData")

load("./Houston/output/kmeans_clustering.RData")

print(clustering$centers)
print(clustering$size)
```

Then, let's visualize the clusters.

```{r cluster visualization, include=FALSE}
clustering$cluster <- as.factor(clustering$cluster)
ggplot(temp, aes(num.death, viol.quantity, color = clustering$cluster)) + geom_point()
```

#### Spectral Clustering

Next, let's try out spectral clustering, which is based on the connectivity between data points.

```{r sprectral clustering, include=FALSE}
temp <- complete.active.quarters %>% filter(active==TRUE)
set.seed(20)

sc.model <- specc(temp[, 6:ncol(temp)], centers=3)
#save(sc.model, file="sc_model.RData")

load("./Houston/output/sc_model.RData")

#plot(temp[, 6:ncol(temp)], col=sc.model, pch=4) # too many features, very slow
plot(temp[, 6], col=sc.model, pch=4) # plot num.days.lost cluster

```

The dataset is highly skewed, so K-means does not perform well as expected since it uses means to estimate centers. On the other hand, spectral clustering uses affinity between data points to cluster. However, the performance is poor as well.


### Markov Chain

```{r markov, include=FALSE}
# assign low, mid, high risk labels
risk.labels <- c("low", "mid", "high")
cluster.to.risk.label <- sort.int(clustering$centers[,1], index.return = TRUE)$ix

# Melted format containing quarter-year, mine_id allowing for markov chain analysis ----
temp$cluster <- clustering$cluster
temp$date <- paste(as.character(temp$year), "-" , as.character(temp$quarter))
temp2 <- temp %>% select(mine_id, cluster, date)
melted <- melt(temp2, id=c("mine_id", "date"))
aggregate_cluster_seq <- cast(melted, mine_id~date)

# Markov Chain function ----
markov <- function(x){
  x <- as.integer(x)
  cluster_seq_raw <- x[!is.na(x)]
  if(length(cluster_seq_raw) > 1){
    cluster_sequence <- cluster_seq_raw[2:length(cluster_seq_raw)]
    cur_markov_chain <- createSequenceMatrix(cluster_sequence, sanitize = FALSE)
    return (list("mine_id" = x[1], 
                 "markov_chain" = cur_markov_chain, 
                 "num_rows" = length(cluster_seq_raw),
                 "low" = cluster.to.risk.label[1] %in% cluster_sequence, 
                 "mid" = cluster.to.risk.label[2] %in% cluster_sequence, 
                 "high" = cluster.to.risk.label[3] %in% cluster_sequence
    )
    )
  }
  return (list("mine_id" = x[1], 
               "markov_chain" = NA, 
               "num_rows" = 0,
               "low" = FALSE, "mid" = FALSE, "high" = FALSE
  )
  )
}

# create mine-date transition matrices ----
result<-apply(aggregate_cluster_seq, 1, markov)

# Create Overall transition matrix ----
markov_chains_ind <- sapply(result, "[[", 2)

#adding matrix of different dimentions, source: http://stackoverflow.com/questions/13571359/join-and-sum-not-compatible-matrices
add_matrices_1 <- function(...) {
  a <- list(...)
  cols <- sort(unique(unlist(lapply(a, colnames))))
  rows <- sort(unique(unlist(lapply(a, rownames))))
  out <- array(0, dim=c(length(rows), length(cols)), dimnames=list(rows,cols))
  for(M in a) { out[rownames(M), colnames(M)] <- out[rownames(M), colnames(M)] + M }
  return(out)
}

final_transition<- matrix(rep(0,times= 9), ncol = 3, nrow = 3)
colnames(final_transition) <- c(1,2,3)
rownames(final_transition) <- c(1,2,3)

for(i in 1:length(markov_chains_ind)){
  final_transition <- add_matrices_1(final_transition,markov_chains_ind[[i]])
}
print("Tranition matrix frequency")
final_transition

final_markov<- matrix(rep(0,times= 9), ncol = 3, nrow = 3)
for(i in 1:3){
  final_markov[i,] <- final_transition[i,]/sum(final_transition[i,])  
}

print("Transition matrix probability")
sorted.final.markov <- final_markov[cluster.to.risk.label, cluster.to.risk.label]
colnames(sorted.final.markov) <- risk.labels
rownames(sorted.final.markov) <- risk.labels
print(sorted.final.markov)
```

### SVM

We try to classify risky mines using SVM.

```{r svm, include=FALSE}
# check train and test data
head(labeled.data.lost.train)
head(labeled.data.lost.result)

# convert label from `bool` to `int`
labeled.data.lost.train <- labeled.data.lost.train %>% mutate(severe = severe*1)
labeled.data.lost.result <- labeled.data.lost.result %>% mutate(severe = severe*1)

svm.model <- svm(severe ~ 
                   last.quarter.lost+last.year.lost+last.three.years.lost
                 +last.quarter.restrict+last.year.restrict+last.three.years.restrict
                 +last.quarter.viol+last.year.viol+last.three.years.viol
                 +last.quarter.death+last.year.death+last.three.years.death,
                 data=labeled.data.lost.train, method="C-classification", kernel="radial")
summary(svm.model)

# save the computationally costly svm model
#save(svm.model, file="./Houston/data/svm_radial.RData")
load("./Houston/output/svm_radial.RData")

tune_out <- tune.svm(severe~.,
                     data=labeled.data.lost.train,
                     gamma=10^(-3:3),
                     cost=c(0.01,0.1,1,10,100,1000) ) # e1071 tune functions have internal iteration limits, in this case, it is better to use another package

compareTable <- table (labeled.data.lost.train$severe, predict(svm.model))  # tabulate
mean(labeled.data.lost.train$severe != predict(svm.model)) # misclassification rate

pred.train <- predict(svm.model, labeled.data.lost.train) # Returns value from 0 to 1. Needs to be thresholded to give 0 or 1
mean(pred.train==labeled.data.lost.train$severe) # classification rate
```

It turns out SVM is not a good option for classifying highly skewed data. `tune.svm` does not work for this dataset due to its large size. Current SVM model with RBF kernel gives 100% misclassification rate.

### Random Forest

Random forest is self-bagging algorithm, which never overfits no matter what number of trees given. It is fast and performs well in most cases. We try to use random forests to classify the data.

```{r randomForest, include=FALSE}
# reformat labeled.data.lost.train$severe to factors so that `randomForest`
# can do a classification
labeled.data.lost.train$severe <- labeled.data.lost.train$severe %>% factor
labeled.data.lost.result$severe <- labeled.data.lost.result$severe %>% factor
set.seed(20)

# Note: nrow of `labled.data.lost.train` and `labeled.data.lost.result` are not equal
# so, we need to modify the longer one of them in order to run `predict` function.
# Let's do it on `labeled.data.lost.train`.
labeled.data.lost.train.trimmed <- labeled.data.lost.train[9:nrow(labeled.data.lost.train), ]

ptm <- proc.time() # start clock to time the process
rf.model <- randomForest(severe ~ last.quarter.lost+last.year.lost+last.three.years.lost
                 +last.quarter.restrict+last.year.restrict+last.three.years.restrict
                 +last.quarter.viol+last.year.viol+last.three.years.viol
                 +last.quarter.death+last.year.death+last.three.years.death,
                 data = labeled.data.lost.train.trimmed,
                 strata = mind_id,
                 nodesize = 240, # tuning parameter; does not affect much; may improve speed a little
                 ntree=200, # tuning parameter; no effect
                 mtry = 2 # tuning parameter; improves OOB error a little; see `rf.tune.mtry`
                 )
proc.time() - ptm # stop the clock

#save(rf.model, file="rf_model.RData") # base model, without any tuning
#save(rf.model, file="rf_model_tuned.RData") # tuned model

load("./Houston/output/rf_model.RData")
load("./Houston/output/rf_model_tuned.RData")

rf.tune.mtry <- tuneRF(labeled.data.lost.train[, c('last.quarter.lost','last.year.lost',
                                                'last.three.years.lost','last.quarter.restrict',
                                                'last.year.restrict', 'last.three.years.restrict',
                                                'last.quarter.viol','last.year.viol',
                                                'last.three.years.viol', 'last.quarter.death',
                                                'last.year.death', 'last.year.death',
                                                'last.three.years.death')],
                       labeled.data.lost.train[, 'severe']
                       )

#save(rf.model, file="rf_tune_mtry.RData") # mtry tuning results

pred <- predict(rf.model, data=labeled.data.lost.result)
freq <- table(pred, labeled.data.lost.result$severe)
sum(diag(freq))/sum(freq) # report accuracy

plot(rf.model) # plot the error rate vs number of trees

```

It turns out that random forest can classify the problem in a reasonable time, however, the false predication of true events is 0.88 after tuning. The OOB error is 1.1%. tuning does not improve the performance significantly.


### Ensemble Learning

Now, let's try an ensemble learning method, which is a combination of single learning algorithms. In this case, let's try svm and random forest and see if the ensemble learning could improve the performance.

Read [this post](https://www.datacamp.com/community/tutorials/ensemble-r-machine-learning) first.

```{r ensemble, include=FALSE}
# encode response variable for superlearner
y <- as.numeric(labeled.data.lost.train$severe)-1
ytest <- as.numeric(labeled.data.lost.result$severe)-1

# separate predictors
x <- data.frame(labeled.data.lost.train[, c('last.quarter.lost','last.year.lost',
                                            'last.three.years.lost','last.quarter.restrict',
                                            'last.year.restrict', 'last.three.years.restrict',
                                            'last.quarter.viol','last.year.viol',
                                            'last.three.years.viol', 'last.quarter.death',
                                            'last.year.death', 'last.year.death',
                                            'last.three.years.death')])
xtest <- data.frame(labeled.data.lost.result[, c('last.quarter.lost','last.year.lost',
                                            'last.three.years.lost','last.quarter.restrict',
                                            'last.year.restrict', 'last.three.years.restrict',
                                            'last.quarter.viol','last.year.viol',
                                            'last.three.years.viol', 'last.quarter.death',
                                            'last.year.death', 'last.year.death',
                                            'last.three.years.death')])

# TODO: 1. try default params first
#       2. then try tuned params with ksvm and randomForest
#       3. try some other learners such as glmnet
ensemble.model <- SuperLearner(y,
                               x,
                               family=binomial(),
                               SL.library=list("SL.randomForest", # TODO: use ranger, SL.randomForest will be removed from SuperLearner
                                               "SL.ksvm") 
                               )

SL.randomForest.tune <- function(...){
      SL.randomForest(..., num.trees=200, mtry=2, nodeside=240)
    }

ensemble.model.tune <- SuperLearner(y,
                                    x,
                                    family=binomial(),
                                    SL.library=list("SL.ranger", # TODO: do `install.packages("ranger")` first if using `SL.ranger`
                                                    "SL.ksvm",
                                                    "SL.randomForest.tune") 
                                    )

ensemble.model.more <- SuperLearner(y,
                                    x,
                                    family=binomial(),
                                    SL.library=list("SL.ranger", # TODO: need pacakge `ranger`, please do `install.packages("ranger")`
                                                    "SL.ksvm",
                                                    "SL.randomForest.tune",
                                                    "SL.glmnet") # TDOD: need package `glmnet`
                                    )


```
